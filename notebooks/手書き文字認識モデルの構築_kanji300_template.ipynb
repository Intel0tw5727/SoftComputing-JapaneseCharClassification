{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  手書き文字(ひらがな73文字)認識モデルの構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概要\n",
    "MNIST手書き文字認識で0から9までのラベルを持った10種類の画像の分類を行ったが、更に応用して日本語手書き文字認識モデルを構築していく。構築には2種類のモデルを作成し、ひらがな73文字版の分類と、漢字300文字版の分類の2種類を行う。\n",
    "\n",
    "> http://lab.ndl.go.jp/cms/hiragana73  \n",
    "> http://lab.ndl.go.jp/cms/kanji300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像や前処理周りのimport\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 深層学習周りのimport\n",
    "import keras\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD, Adadelta, Adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バージョン確認\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import tqdm as tm\n",
    "print(\"numpy => {}\".format(np.__version__))\n",
    "print(\"matplotlib => {}\".format(matplotlib.__version__))\n",
    "print(\"pandas => {}\".format(pd.__version__))\n",
    "print(\"OpenCV => {}\".format(cv2.__version__))\n",
    "print(\"tqdm => {}\".format(tm.__version__))\n",
    "print(\"scikit-learn => {}\".format(sklearn.__version__))\n",
    "print(\"keras => {}\".format(keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセット文字コード対応表の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_char_df = pd.read_csv(\"../data/kanji_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_char_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 漢字データセット総数: 146,157\n",
    "# 1ラベル約100~1000程度\n",
    "dir_path = \"../data/kanji300\"\n",
    "img_list = []\n",
    "label_list = []\n",
    "\n",
    "for root, dirs, files in tqdm(os.walk(dir_path)):\n",
    "    if len(files) == 0:\n",
    "        labels = dirs\n",
    "        labels_dict = dict(zip([_ for _ in range(len(labels))], labels))\n",
    "    else:\n",
    "        tmp = []\n",
    "        idx = jp_char_df[jp_char_df.dir == root.split(\"/\")[-1]].index.values[0]\n",
    "        for file in files:\n",
    "            img_list.append(cv2.imread(os.path.join(root, file)))\n",
    "            label_list.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ前処理\n",
    "\n",
    "このままでは学習に時間がかかるため、前処理時点で訓練データとテストデータを加工して、使いやすくするために3つの処理を行います。\n",
    "\n",
    "- 画像のグレースケール化\n",
    "- 画像のリサイズ\n",
    "- 画像の2値化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_prop_list = []\n",
    "flag = False\n",
    "for img in tqdm(img_list):\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # グレースケール化\n",
    "    img_resize = cv2.resize(img_gray, (28,28)) # リサイズ\n",
    "    _, img_prop = cv2.threshold(img_resize, 0, 255, \\\n",
    "                                cv2.THRESH_BINARY + cv2.THRESH_OTSU) # 2値化\n",
    "    \n",
    "    if flag == False: \n",
    "        # 初回のみplot\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(18,8))\n",
    "        titles = [\"gray_scaled\", \"resized\", \"binarized\"]\n",
    "        imgs = [img_gray, img_resize, img_prop]\n",
    "        for n, (k, v) in enumerate(zip(titles, imgs)):\n",
    "            ax[n].imshow(v, cmap=\"gray\")\n",
    "            # ax[n].axis(\"off\")\n",
    "            ax[n].set_title(\"{}\".format(k))\n",
    "        flag = True\n",
    "\n",
    "    img_prop_list.append(img_prop)\n",
    "    \n",
    "# img_list はもう使わないのでメモリから開放する\n",
    "del img_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = np.array(img_list), label_list\n",
    "X, y = np.array(img_prop_list), label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.2, random_state=98, shuffle=True)\n",
    "\n",
    "# X_train = X_train.reshape(len(X_train), X_train[0].shape[0], X_train[0].shape[1], 3)\n",
    "X_train = X_train.reshape(len(X_train), X_train[0].shape[0], X_train[0].shape[1], 1)\n",
    "\n",
    "# X_test = X_test.reshape(len(X_test), X_test[0].shape[0], X_test[0].shape[1], 3)\n",
    "X_test = X_test.reshape(len(X_test), X_test[0].shape[0], X_test[0].shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainデータの1枚を確認\n",
    "plt.imshow(X_train[0].reshape(28, 28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testデータの1枚を確認\n",
    "jp_char_df.loc[y_train[0]][\"char\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train.shape -> {}\".format(X_train.shape))\n",
    "print(\"X_test.shape -> {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = len(labels)\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y_train.shape -> {}\".format(len(y_train)))\n",
    "print(\"y_test.shape -> {}\".format(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "\"\"\"\n",
    "model.add()の中にConv2DやMaxPooling2Dをいれてモデルを作ってみよう\n",
    "\n",
    "今回使用する関数一覧\n",
    "\n",
    "model.add(Dense(次元数, activation=活性化関数))\n",
    "model.add(Flatten()) # 畳み込みし終えた後全結合層につなげるときに使おう\n",
    "model.add(Conv2D(フィルター数, \n",
    "                                    kernel_size=(フィルターの縦サイズ, フィルターの横サイズ),\n",
    "                                    activation=活性化関数,\n",
    "                                    input_shape=input_shape)) #input_shapeは最初のみ使用\n",
    "model.add(MaxPooling2D(pool_size=(プーリングの縦サイズ, プーリングの横サイズ))))\n",
    "model.add(Dropout(0から1までの数値)) # 学習するパーセプトロンのうち使用しない割合を設定\n",
    "\n",
    "その他、調べてみて便利な関数があればぜひ追加してみよう\n",
    "\"\"\"\n",
    "\n",
    "model.add(Conv2D(28, kernel_size=(3, 3),activation='relu', padding=\"same\", input_shape=input_shape))\n",
    "model.add(Conv2D(28, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(56, kernel_size=(3, 3),activation='relu', padding=\"same\"))\n",
    "model.add(Conv2D(56, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(112, kernel_size=(3, 3),activation='relu', padding=\"same\"))\n",
    "model.add(Conv2D(112, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Fully connected layer #2\n",
    "model.add(Dense(len(labels), activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "model = model_from_json(open(\"../models/kanji_cnn.json\", \"r\").read())\n",
    "model.load_weights(\"../params/kanji_cnn_best_weight.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルのコンパイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "モデルを評価する関数をmodel.compile()で定義しよう\n",
    "\n",
    "実際にmodel.compileの中にはこのようにします\n",
    "\n",
    "model.compile(loss=誤差関数,\n",
    "             optimizer=最適化関数,\n",
    "             metrics=['accuracy']\n",
    "             )\n",
    "             \n",
    "誤差関数\n",
    "・categorical_crossentropy\n",
    "\n",
    "最適化関数(好きなものを選ぼう)\n",
    "・SGD\n",
    "・Adadelta\n",
    "・Adam\n",
    "・RMSprop\n",
    "\n",
    "評価指標\n",
    "・accuracy\n",
    "\"\"\"\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", # 誤差(損失)関数\n",
    "             optimizer=\"RMSprop\", # 最適化関数\n",
    "             metrics=[\"accuracy\"] # 評価指標\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初期モデル・パラメータを保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_weights_path = '../models/hiragana_cnn_init_weight.hdf5'\n",
    "best_weights_path = '../models/hiragana_cnn_best_weight.hdf5'\n",
    "model.save_weights(init_weights_path, overwrite=True)\n",
    "model.save_weights(best_weights_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../models/hiragana_cnn.json'\n",
    "model_json = model.to_json()\n",
    "open(model_path, 'w').write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習中のコールバックの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_acc', patience=5,mode='max',verbose=1),\n",
    "    ModelCheckpoint(best_weights_path,monitor='val_acc', save_best_only=True, \n",
    "        mode='max',verbose=0)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=512,\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "print(\"Execution time: {0:.2f} [sec]\".format(toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習結果の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].set_title('Training performance (Loss)')\n",
    "ax[0].plot(history.epoch, history.history['loss'], label='loss')\n",
    "ax[0].plot(history.epoch, history.history['val_loss'], label='val_loss')\n",
    "ax[0].set(xlabel='Epoch', ylabel='Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_title('Training performance (Accuracy)')\n",
    "ax[1].plot(history.epoch, history.history['acc'], label='acc')\n",
    "ax[1].plot(history.epoch, history.history['val_acc'], label='val_acc')\n",
    "ax[1].set(xlabel='Epoch', ylabel='Accuracy')\n",
    "ax[1].legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame([history.history[\"loss\"],history.history[\"acc\"],history.history[\"val_loss\"],history.history[\"val_acc\"]])\n",
    "history_df.index = [\"loss\", \"acc\", \"val_loss\", \"val_acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df.T.to_csv(\"../data/learning_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータの可視化\n",
    "fig, ax = plt.subplots(1, 10, figsize=(18, 8))\n",
    "\n",
    "for ii in range(10):\n",
    "    ax[ii].imshow(X_test[ii].reshape(48, 48), cmap='gray') #iiの値+nでn番目以降のテストデータを出力する．\n",
    "    ax[ii].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanji_df = pd.read_csv(\"../data/kanji_table_prop.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測の可視化\n",
    "[kanji_df.loc[y_pred.argmax()][\"char\"] for y_pred in Y_test_pred[:10]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
